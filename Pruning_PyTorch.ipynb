{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pruning_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soniajoseph/Collaborative-Representation-Based-Classification/blob/master/Pruning_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNk2Xgu6UTG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "import copy\n",
        "import pandas as pd "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX_iW6N-sLkF",
        "colab_type": "text"
      },
      "source": [
        "## The Unexpected Effects of Pruning Neural Nets\n",
        "\n",
        "*This project was done as a challenge for [for.ai](for.ai), a multi-disciplinary distributed artificial intelligence research collaboration.*\n",
        "\n",
        "Pruning is deleting connections in a neural net in order to improve generalization and reduce computational resources. Two kinds of pruning exist: weight-pruning, in which the largest weights by absolute value are set to zero; and unit-pruning, in which the smallest neurons are set to zero by a vector-wise metric like L2-norm.\n",
        "\n",
        "Here, I examine the relationship between pruning and accuracy on a vanilla neural net. Before running any experiments, I hypothesize that accuracy for the pruned neural net will slightly rise (due to the regularization), and then have a negative linear correlation with the amount pruned. I also hypothesize that unit-pruning, in deleting entire neurons instead of individual weights, will have a more dramatic negative effect than weight-pruning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZmejOzw8MHS",
        "colab_type": "text"
      },
      "source": [
        "## First, let's load, normalize, and visualize the MNIST dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5QgQR6UUlwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_MNIST():\n",
        "  \"\"\"Function to load and normalize MNIST data\"\"\" \n",
        "  train = torchvision.datasets.MNIST(root='./data', download=True, train=True, transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,)),\n",
        "                                ]))\n",
        "  test = torchvision.datasets.MNIST(root='./data', download=True, train=False, transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,)),\n",
        "                                ]))\n",
        "  print(\"MNIST datset loaded and normalized.\")\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=train, shuffle=True, batch_size=100)\n",
        "  test_loader = torch.utils.data.DataLoader(dataset=test, shuffle=False, batch_size=100)\n",
        "  print(\"PyTorch DataLoaders loaded.\")\n",
        "  return train, test, train_loader, test_loader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X3rzpxCXv4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_MNIST(train_loader):\n",
        "  \"\"\"Function to visualize data given a DataLoader object\"\"\"\n",
        "  dataiter = iter(train_loader)\n",
        "  images, labels = dataiter.next()\n",
        "  print(\"image shape:\", images.shape, \"\\n label shape:\", labels.shape)\n",
        "  # visualize data\n",
        "  fig, ax = plt.subplots(2,5)\n",
        "  for i, ax in enumerate(ax.flatten()):\n",
        "      im_idx = np.argwhere(labels == i)[0][0]\n",
        "      plottable_image = images[im_idx].squeeze()\n",
        "      ax.imshow(plottable_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awglncFta6ga",
        "colab_type": "code",
        "outputId": "056ad7ae-0460-48b8-f6ee-3acdd70c99c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# load and visualize MNISt\n",
        "train, test, train_loader, test_loader = load_MNIST()\n",
        "visualize_MNIST(train_loader)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MNIST datset loaded and normalized.\n",
            "PyTorch DataLoaders loaded.\n",
            "image shape: torch.Size([100, 1, 28, 28]) \n",
            " label shape: torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAADSCAYAAABXT0tTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avoio0W48Yzi",
        "colab_type": "text"
      },
      "source": [
        "## Now let's build a vanilla neural net with four hidden layers without pruning.\n",
        "\n",
        "We'll keep things simple and leave out biases, convolutions, and pooling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnUxUg7iU4yn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  \"\"\"A non-sparse neural network with four hidden fully-connected layers\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    self.input_layer = nn.Linear(784, 1000, bias=False)\n",
        "    self.hidden1_layer = nn.Linear(1000, 1000, bias=False)\n",
        "    self.hidden2_layer = nn.Linear(1000, 500, bias=False)\n",
        "    self.hidden3_layer = nn.Linear(500, 200, bias=False)\n",
        "    self.hidden4_layer = nn.Linear(200, 10, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.input_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden1_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden2_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden3_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden4_layer(x)\n",
        "    output = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqA-fHwd8toX",
        "colab_type": "text"
      },
      "source": [
        "## Let's train our vanilla neural net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJcwD5-NeJBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, epochs=3, learning_rate=0.001):\n",
        "  \"\"\"Function to train a neural net\"\"\"\n",
        "\n",
        "  lossFunction = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  time0 = time()\n",
        "  total_samples = 0 \n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(\"Starting epoch\", e)\n",
        "    total_loss = 0\n",
        "\n",
        "    for idx, (images,labels) in enumerate(train_loader):\n",
        "      images = images.view(images.shape[0],-1) # flatten\n",
        "      optimizer.zero_grad() # forward pass\n",
        "      output = model(images)\n",
        "      loss = lossFunction(output,labels) # calculate loss\n",
        "      loss.backward() # backpropagate\n",
        "      optimizer.step() # update weights\n",
        "\n",
        "      total_samples += labels.size(0)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      if idx % 100 == 0:\n",
        "        print(\"Running loss:\", total_loss)\n",
        "\n",
        "  final_time = (time()-time0)/60 \n",
        "  print(\"Model trained in \", final_time, \"minutes on \", total_samples, \"samples\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51kd-weohlPf",
        "colab_type": "code",
        "outputId": "a1df4e97-f5cc-4a7c-f920-2fc21d7c5d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "model = Net()\n",
        "train(model, train_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 0\n",
            "Running loss: 2.3025763034820557\n",
            "Running loss: 79.13857460021973\n",
            "Running loss: 113.31019903719425\n",
            "Running loss: 138.79075822234154\n",
            "Running loss: 160.88071709871292\n",
            "Running loss: 181.42165705934167\n",
            "Starting epoch 1\n",
            "Running loss: 0.22079169750213623\n",
            "Running loss: 15.352100413292646\n",
            "Running loss: 30.58726792782545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArL0CsYL83AB",
        "colab_type": "text"
      },
      "source": [
        "## Now we'll test our vanilla neural net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u61CAZip1UNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_loader):\n",
        "  \"\"\"Test neural net\"\"\"\n",
        "\n",
        "  correct = 0\n",
        "  total = 0 \n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, (images, labels) in enumerate(test_loader):\n",
        "      images = images.view(images.shape[0],-1) # flatten\n",
        "      output = model(images)\n",
        "      values, indices = torch.max(output.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (labels == indices).sum().item()\n",
        "\n",
        "    acc = correct / total * 100\n",
        "    # print(\"Accuracy: \", acc, \"% for \", total, \"training samples\")\n",
        "\n",
        "  return acc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uAz77644j08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = test(model, test_loader)\n",
        "print(\"The accuracy of our vanilla NN is\", acc, \"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8QEw_mWGA3D",
        "colab_type": "text"
      },
      "source": [
        "## A ~96% accuracy for our vanilla NN seems reasonable. Now let's do some weight and unit pruning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3EavCDY4mOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def sparsify_by_weights(model, k):\n",
        "  \"\"\"Function that takes un-sparsified neural net and does weight-pruning\n",
        "  by k sparsity\"\"\"\n",
        "\n",
        "  # make copy of original neural net\n",
        "  sparse_m = copy.deepcopy(model)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, i in enumerate(sparse_m.parameters()): \n",
        "      if idx == 4: # skip last layer of 5-layer neural net\n",
        "        break \n",
        "      # change tensor to numpy format, then set appropriate number of smallest weights to zero\n",
        "      layer_copy = torch.flatten(i)\n",
        "      layer_copy = layer_copy.detach().numpy()\n",
        "      indices = abs(layer_copy).argsort() # get indices of smallest weights by absolute value\n",
        "      indices = indices[:int(len(indices)*k)] # get k fraction of smallest indices \n",
        "      layer_copy[indices] = 0 \n",
        "\n",
        "      # change weights of model\n",
        "      i = torch.from_numpy(layer_copy)\n",
        "  \n",
        "  return sparse_m  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTb5ptg80hqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def l2(array):\n",
        "  return np.sqrt(np.sum([i**2 for i in array]))\n",
        "\n",
        "def sparsify_by_unit(model, k):\n",
        "  \"\"\"Creates a k-sparsity model with unit-level pruning that sets columns with smallest L2 to zero.\"\"\"\n",
        "  \n",
        "  # make copy of original neural net\n",
        "  sparse_m = copy.deepcopy(model)\n",
        "\n",
        "  for idx, i in enumerate(sparse_m.parameters()):\n",
        "    if idx == 4: # skip last layer of 5-layer neural net\n",
        "      break\n",
        "    layer_copy = i.detach().numpy()\n",
        "    indices = np.argsort([l2(i) for i in layer_copy])\n",
        "    indices = indices[:int(len(indices)*k)]\n",
        "    layer_copy[indices] = 0\n",
        "    i = torch.from_numpy(layer_copy)\n",
        "  \n",
        "  return sparse_m \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX21TSoq3udv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pruning_accuracies(model, prune_type):\n",
        "  \"\"\" Takes a model and prune type (\"weight\" or \"unit\") and returns a DataFrame of pruning accuracies for given sparsities.\"\"\"\n",
        "\n",
        "  df = pd.DataFrame({\"sparsity\": [], \"accuracy\": []})\n",
        "  sparsities = [0.0, 0.25, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.97, 0.99]\n",
        "\n",
        "  for s in sparsities:\n",
        "    if prune_type == \"weight\":\n",
        "      new_model = sparsify_by_weights(model, s)\n",
        "    elif prune_type == \"unit\":\n",
        "      new_model = sparsify_by_unit(model, s)\n",
        "    else:\n",
        "      print(\"Must specify prune-type.\")\n",
        "      return \n",
        "    acc = test(new_model, test_loader)\n",
        "    df = df.append({\"sparsity\": s, \"accuracy\": acc}, ignore_index=True)\n",
        "\n",
        "  return df "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIWGnpQZHYSi",
        "colab_type": "text"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WgshrNX5Is2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_weight = get_pruning_accuracies(model, \"weight\")\n",
        "df_unit = get_pruning_accuracies(model, \"unit\")\n",
        "\n",
        "print(\"Accuracies for Weight Pruning\")\n",
        "print(df_weight)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Accuracies for Unit Pruning\")\n",
        "print(df_unit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVBWjZMMDglS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.title(\"Accuracy vs Sparsity\")\n",
        "plt.plot(df_unit[\"sparsity\"], df_unit[\"accuracy\"], label=\"Unit-pruning\")\n",
        "plt.plot(df_weight[\"sparsity\"], df_weight[\"accuracy\"], label=\"Weight-pruning\")\n",
        "plt.xlabel(\"Sparsity (as fraction)\")\n",
        "plt.ylabel(\"% Accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj0L8f_A7m3b",
        "colab_type": "text"
      },
      "source": [
        "## Using New-Found Sparsity to Speed up the Neural Net\n",
        "\n",
        "We"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNGO5Ih9Hv52",
        "colab_type": "text"
      },
      "source": [
        "## Discussion (pre-literature review)\n",
        "\n",
        "Clearly, my hypothesis that accuracy will rise and then negatively correlate in a roughly linear way with pruning was incorrect. The figure instead shows a dramatic nonlinear relationship between accuracy and pruning. Accuracy remains roughly constant until dropping off at about 75% sparsity for weight-pruning and until 70% sparsity for unit-pruning. My hypothesis that unit-pruning impacts accuracy more dramatically than weight-pruning held up.\n",
        "\n",
        "These results are fascinating: Less than 25% of the neural net represents important information about its function. The data also suggest that accuracy may slightly increase with a light amount of pruning (~30%), although I would run on more iterations with a larger dataset to be sure. It would make sense that keeping the net's smaller weights reduces its generalization.\n",
        "\n",
        "\n",
        "## Literature Review\n",
        "\n",
        "Let's turn to existing papers to get a better grasp on the pruning phenomenon. \n",
        "\n",
        "In [\"The Lottery Ticket Hypothesis\"](https://arxiv.org/pdf/1803.03635.pdf), the authors put forth the idea:\n",
        "\n",
        "> \"A randomly-initialized, dense neural network contains a subnetwork that is initialized such that—when trained in isolation—it can match the test accuracy of the\n",
        "original network after training for at most the same number of iterations.\"\n",
        ">\n",
        "\n",
        "Pruning the network automatically finds the \"winning ticket\" subnetwork, whose accuracy is comparable to that of the fully trained net. The idea is similar to the one proposed in [\"Optimal Brain Damage\"](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf), in which the authors prune a network based on second-derivative information.\n",
        "\n",
        "## Further Directions\n",
        "\n",
        "Questions remain. The theory behind the pruning-accuracy relationship remains unclear, and it is an ongoing area of research. Are there other ways of finding these \"winning ticket\" subnetworks besides pruning (e.g. directly from the objective function)? Why does one have to train a largely overparameterized network first for the winning ticket to arise? Can we find winning ticket subnetwork before training the full network (i.e. during training)? \n",
        "\n",
        "I am also curious if CNNs, RNNs, and ResNets show the same relationship between pruning and accuracy as the vanilla NN examined here. I am interested in the effect of pruning the weights by magnitude of the entire net (opposed to layer by layer), and using magnitude measures other than absolute value and L2-norm. And what about deleting the largest weights and neurons, opposed to the smallest?\n",
        "\n",
        "Lastly, I am interested in using pruning in artificial nets as a computational model for [synaptic pruning](https://en.wikipedia.org/wiki/Synaptic_pruning) with microglia in biological brains. It is possible that synaptic pruning conserves biological resources, improves brain functioning, or both. "
      ]
    }
  ]
}